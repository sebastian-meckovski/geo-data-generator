{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a54fdc5f-dbe6-4ce8-9bb0-04e1590ac82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "languages = ['pl', 'lt', 'ru', 'hu', 'en', 'fr']\n",
    "\n",
    "global_cities_path = 'allCountries.txt'\n",
    "alternate_names_path = 'alternateNamesV2.txt'\n",
    "admin1_codes_path = 'admin1CodesASCII.txt'\n",
    "\n",
    "global_cities_headers = [\n",
    "    'geoname_id', 'name', 'ascii_name', 'alternate_names', 'latitude', 'longitude',\n",
    "    'feature_class', 'feature_code', 'country_code', 'cc2', 'admin1_code',\n",
    "    'admin2_code', 'admin3_code', 'admin4_code', 'population', 'elevation',\n",
    "    'dem', 'timezone', 'modification_date'\n",
    "]\n",
    "\n",
    "global_cities_headers_usecols = [\n",
    "    'geoname_id', 'name', 'ascii_name', 'latitude', 'longitude',\n",
    "    'feature_code', 'country_code', 'admin1_code', 'population'\n",
    "]\n",
    "\n",
    "# Define the data types for the columns in the global cities file\n",
    "global_cities_dtype = {\n",
    "    'geoname_id': 'Int64', 'name': str, 'asciiname': str, 'alternatenames': str,\n",
    "    'latitude': float, 'longitude': float, 'feature_class': str, 'feature_code': str,\n",
    "    'country_code': str, 'cc2': str, 'admin1_code': str, 'admin2_code': str,\n",
    "    'admin3_code': str, 'admin4_code': str, 'population': 'Int64', 'elevation': float,\n",
    "    'dem': float, 'timezone': str, 'modification_date': str\n",
    "}\n",
    "\n",
    "# Define the column headers for the alternate names file\n",
    "alternate_names_headers = [\n",
    "    'alternate_name_id', 'geoname_id', 'iso_language', 'alternate_name',\n",
    "    'is_preferred_name', 'is_short_name', 'is_colloquial', 'is_historic', \n",
    "    'from', 'to'\n",
    "]\n",
    "\n",
    "alternate_names_headers_usecols = [\n",
    "    'geoname_id', 'iso_language', 'alternate_name',\n",
    "    'is_preferred_name', 'is_short_name', 'is_colloquial', 'is_historic'\n",
    "]\n",
    "\n",
    "alternate_names_dtype = {\n",
    "    'alternate_name_id': 'Int64', 'geoname_id': 'Int64', 'iso_language': str, 'alternate_name': str,\n",
    "    'is_preferred_name': 'boolean', 'is_short_name': 'boolean', 'is_colloquial': 'boolean', 'is_historic': 'boolean',\n",
    "    'from': str, 'to': str\n",
    "}\n",
    "\n",
    "admin1_codes_headers = [\n",
    "    'code', 'name', 'name_ascii', 'geoname_id_admin1'\n",
    "]\n",
    "\n",
    "admin1_codes_usecols = ['code', 'name', 'geoname_id_admin1']\n",
    "\n",
    "admin1_codes_dtype = {\n",
    "    'code': str, 'name': str, 'name_ascii': str, 'geoname_id_admin1': 'Int64'\n",
    "}\n",
    "\n",
    "alternate_names_df = pd.read_csv(alternate_names_path, sep='\\t', header=None, names=alternate_names_headers, dtype=alternate_names_dtype, low_memory=False, keep_default_na=False, na_values='', encoding='utf-8', usecols=alternate_names_headers_usecols)\n",
    "cities_df = pd.read_csv(global_cities_path, sep='\\t', header=None, names=global_cities_headers, dtype=global_cities_dtype, low_memory=False, keep_default_na=False, na_values='', encoding='utf-8', usecols=global_cities_headers_usecols)\n",
    "admin1_codes_df = pd.read_csv(admin1_codes_path, sep='\\t', header=None, names=admin1_codes_headers, dtype=admin1_codes_dtype, low_memory=False, keep_default_na=False, na_values='', encoding='utf-8', usecols=admin1_codes_usecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75b6ae0b-9a7c-4810-b8e6-360a2aea72f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill <NA> values with False for the specified columns\n",
    "alternate_names_df[['is_preferred_name', 'is_short_name', 'is_colloquial', 'is_historic']] = \\\n",
    "    alternate_names_df[['is_preferred_name', 'is_short_name', 'is_colloquial', 'is_historic']].fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c714898b-b6d3-4403-ab67-f0bcd7ae383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate countries dataset\n",
    "countries_df = cities_df[cities_df['feature_code'].isin(['PCLI', 'PCLS', 'PCLIX', 'TERR', 'PCLD', 'PCL', 'PCLF'])].rename(columns={'name': 'name_country'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea49b0a3-29cc-4c8a-b371-dba1ed154c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "geoname_id      486\n",
       "name            486\n",
       "ascii_name      486\n",
       "latitude        486\n",
       "longitude       486\n",
       "feature_code    486\n",
       "country_code    486\n",
       "admin1_code     484\n",
       "population      486\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_codes = [\n",
    "    'PPLA2', 'PPLA', 'PPLC', 'PPL', 'PPLW',\n",
    "    'PPLG', 'PPLL', 'PPLS', 'PPLF', 'PPLR'\n",
    "]\n",
    "\n",
    "filtered_cities_df = cities_df[cities_df['feature_code'].isin(feature_codes) & (cities_df['population'] >= 20000)]\n",
    "\n",
    "filtered_cities_df[filtered_cities_df['population'] > 1000000].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e376bd1c-48de-4d3a-8e7b-573d7ea3e954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the DataFrames on the country code\n",
    "cities_with_country = pd.merge(filtered_cities_df, countries_df[['geoname_id', 'name_country', 'country_code']], on='country_code', how='left', suffixes=('_city', '_country'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70f57f97-c1dc-4af8-9922-3fe63e5fc6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include first-order administrative division in cities_with_country_table\n",
    "cities_with_country['admin1_geocode'] = cities_with_country['country_code'] + '.' + cities_with_country['admin1_code']\n",
    "\n",
    "cities_with_country_admin1_geocodes = pd.merge(cities_with_country, admin1_codes_df[['code', 'name', 'geoname_id_admin1']], right_on='code',\n",
    "                                               left_on='admin1_geocode', how='left',  suffixes=('_city', '_admin1')).drop('code', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40c48bb5-e53b-4f00-831b-db755dd5865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the admin_area column if the city name is unique within a country. Keep it if multiple cities have the same name in the country.\n",
    "cities_with_country_admin1_geocodes[\"city_count\"] = cities_with_country_admin1_geocodes.groupby([\"geoname_id_country\", \"name_city\"])[\"geoname_id_city\"].transform(\"count\")\n",
    "cities_with_country_admin1_geocodes[\"geoname_id_admin1\"] = cities_with_country_admin1_geocodes.apply(lambda row: row[\"geoname_id_admin1\"] if row[\"city_count\"] > 1 else np.nan, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b032a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geohash\n",
    "\n",
    "def add_geohash(row):\n",
    "  \"\"\"Calculates the geohash for a given latitude and longitude.\"\"\"\n",
    "  return geohash.encode(row['latitude'], row['longitude'], precision=12)\n",
    "\n",
    "cities_with_country_admin1_geocodes['geohash'] = cities_with_country_admin1_geocodes.apply(add_geohash, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82b7951b-52ef-4fed-86ca-760af3bff147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_radius(population):\n",
    "  if 0 <= population < 50000:\n",
    "    return 400\n",
    "  elif 50000 <= population < 100000:\n",
    "    return 800\n",
    "  elif 100000 <= population < 500000:\n",
    "    return 2000\n",
    "  elif 500000 <= population < 1000000:\n",
    "    return 4000\n",
    "  elif 1000000 <= population < 5000000:\n",
    "    return 6000\n",
    "  elif 5000000 <= population < 10000000:\n",
    "    return 14000\n",
    "  else: \n",
    "    return 16000\n",
    "\n",
    "cities_with_country_admin1_geocodes['estimated_radius'] = cities_with_country_admin1_geocodes['population'].apply(calculate_radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a0531ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from pyproj import CRS, Transformer\n",
    "from shapely.geometry import Point\n",
    "from shapely.ops import transform\n",
    "\n",
    "\n",
    "def geodesic_point_buffer(lat, lon, distance):\n",
    "    # Azimuthal equidistant projection\n",
    "    aeqd_proj = CRS.from_proj4(\n",
    "        f\"+proj=aeqd +lat_0={lat} +lon_0={lon} +x_0=0 +y_0=0\")\n",
    "    tfmr = Transformer.from_proj(aeqd_proj, aeqd_proj.geodetic_crs)\n",
    "    buf = Point(0, 0).buffer(distance)  # distance in metres\n",
    "    return transform(tfmr.transform, buf)\n",
    "\n",
    "points_df = cities_with_country_admin1_geocodes\n",
    "\n",
    "# Convert the points to circles by buffering them\n",
    "points_buffer_gdf = gpd.GeoDataFrame(\n",
    "    points_df,\n",
    "    geometry=points_df.apply(\n",
    "        lambda row : geodesic_point_buffer(row.latitude, row.longitude, row.estimated_radius), axis=1\n",
    "    ),\n",
    "    crs=4326,\n",
    ")\n",
    "\n",
    "# Determine the intersecting city buffers (result includes self-intersections)\n",
    "intersecting_gdf = points_buffer_gdf.sjoin(points_buffer_gdf)\n",
    "\n",
    "intersecting_larger_population_df = intersecting_gdf.loc[\n",
    "    (intersecting_gdf.population_left < intersecting_gdf.population_right) \n",
    "    & (intersecting_gdf.population_left < 2000000)  # New condition\n",
    "]\n",
    "\n",
    "# Remove the city buffers that intersect with a larger population city buffer\n",
    "cities_with_country_admin1_geocodes = points_buffer_gdf[\n",
    "    ~points_buffer_gdf.index.isin(intersecting_larger_population_df.index) \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c566f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from math import radians, cos, sin, asin, sqrt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb07236-c70e-4d0b-b985-9b61ea2be296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to combined_data.json\n"
     ]
    }
   ],
   "source": [
    "# Filter alternate_names_df for French names\n",
    "import json\n",
    "\n",
    "def determine_priority(row):\n",
    "    if row['is_preferred_name'] == True and row['is_short_name'] == False and row['is_colloquial'] == False and row['is_historic'] == False:\n",
    "        return 1\n",
    "    elif row['is_preferred_name'] == False and row['is_short_name'] == False and row['is_colloquial'] == False and row['is_historic'] == False:\n",
    "        return 2\n",
    "    elif row['is_preferred_name'] == False and row['is_short_name'] == True and row['is_colloquial'] == False and row['is_historic'] == False:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "    \n",
    "def check_names_city_country(row):\n",
    "    name = str(row['alternate_name_city']).lower().strip()\n",
    "    country = str(row['alternate_name_country']).lower().strip()\n",
    "    return country in name\n",
    "\n",
    "def check_names_city_admin1(row):\n",
    "    name = str(row['alternate_name_city']).lower().strip()\n",
    "    admin1 = str(row['alternate_name_admin1']).lower().strip()\n",
    "    return name in admin1 or admin1 in name\n",
    "\n",
    "def check_names_admin1_country(row):\n",
    "    country = str(row['alternate_name_country']).lower().strip()\n",
    "    admin1 = str(row['alternate_name_admin1']).lower().strip()\n",
    "    return country in admin1 or admin1 in country\n",
    "\n",
    "# Initialize an empty dictionary to store the combined data\n",
    "combined_data = {}\n",
    "\n",
    "for language in languages:\n",
    "    filtered_alternate_names = alternate_names_df[alternate_names_df['iso_language'] == language].copy()\n",
    "    filtered_alternate_names['priority'] = filtered_alternate_names.apply(determine_priority, axis=1)\n",
    "    filtered_alternate_names.sort_values(by=['priority', 'geoname_id'], inplace=True)\n",
    "    filtered_alternate_names = filtered_alternate_names.groupby('geoname_id').first().reset_index()\n",
    "\n",
    "    cities_with_country_admin1_alternates = pd.merge(cities_with_country_admin1_geocodes, filtered_alternate_names[['geoname_id', 'alternate_name']], \n",
    "                                                     how='left', left_on='geoname_id_city', right_on='geoname_id').drop('geoname_id', axis=1)\n",
    "    cities_with_country_admin1_alternates['alternate_name'] = cities_with_country_admin1_alternates['alternate_name'].fillna(\n",
    "        cities_with_country_admin1_alternates['ascii_name']\n",
    "    )\n",
    "\n",
    "    cities_with_country_admin1_alternates = pd.merge(cities_with_country_admin1_alternates, filtered_alternate_names[['geoname_id', 'alternate_name']], \n",
    "                                                     how='left', left_on='geoname_id_admin1', right_on='geoname_id', suffixes=('_city','_admin1')).drop('geoname_id', axis=1)\n",
    "    cities_with_country_admin1_alternates = pd.merge(cities_with_country_admin1_alternates, filtered_alternate_names[['geoname_id', 'alternate_name']], \n",
    "                                                     how='left', left_on='geoname_id_country', right_on='geoname_id').drop('geoname_id', axis=1).rename(columns={'alternate_name': 'alternate_name_country'})\n",
    "    cities_with_country_admin1_alternates['alternate_name_country'] = cities_with_country_admin1_alternates['alternate_name_country'].fillna(\n",
    "        cities_with_country_admin1_alternates['name_country']\n",
    "    )\n",
    "\n",
    "    country_names_indices_to_remove = cities_with_country_admin1_alternates[\n",
    "        cities_with_country_admin1_alternates.apply(check_names_city_country, axis=1)\n",
    "    ].index\n",
    "    cities_with_country_admin1_alternates.loc[country_names_indices_to_remove, 'alternate_name_country'] = np.nan\n",
    "\n",
    "    admin1_names_indices_to_remove = cities_with_country_admin1_alternates[\n",
    "        cities_with_country_admin1_alternates.apply(check_names_city_admin1, axis=1)\n",
    "    ].index \n",
    "    cities_with_country_admin1_alternates.loc[admin1_names_indices_to_remove, 'alternate_name_admin1'] = np.nan\n",
    "\n",
    "    admin1_names_vs_country_indices_to_remove = cities_with_country_admin1_alternates[\n",
    "        cities_with_country_admin1_alternates.apply(check_names_admin1_country, axis=1)\n",
    "    ].index \n",
    "    cities_with_country_admin1_alternates.loc[admin1_names_vs_country_indices_to_remove, 'alternate_name_admin1'] = np.nan\n",
    "\n",
    "    # Iterate through the rows and update the combined_data dictionary\n",
    "    for _, row in cities_with_country_admin1_alternates.iterrows():\n",
    "        geoname_id_city = row['geoname_id_city']\n",
    "        if geoname_id_city not in combined_data:\n",
    "            combined_data[geoname_id_city] = {\n",
    "                'geoname_id_city': geoname_id_city,\n",
    "                'latitude': row['latitude'],\n",
    "                'longitude': row['longitude'],\n",
    "                'geohash': row['geohash'],\n",
    "                'country_code': row['country_code'],\n",
    "                'population': row['population'],\n",
    "                'estimated_radius': row['estimated_radius'],\n",
    "                'name': {}\n",
    "            }\n",
    "        combined_data[geoname_id_city]['name'][language] = {\n",
    "            'city': row['alternate_name_city'] if pd.notna(row['alternate_name_city']) else None,\n",
    "            'admin1': row['alternate_name_admin1'] if pd.notna(row['alternate_name_admin1']) else None,\n",
    "            'country': row['alternate_name_country'] if pd.notna(row['alternate_name_country']) else None\n",
    "        }\n",
    "\n",
    "# Convert the combined_data dictionary to a list\n",
    "nested_json_list = list(combined_data.values())\n",
    "\n",
    "# Save the nested JSON to a file\n",
    "with open('combined_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(nested_json_list, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Data saved to combined_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4508a051",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'folium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfolium\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_popup_content\u001b[39m(row):\n\u001b[0;32m      4\u001b[0m     popup_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'folium'"
     ]
    }
   ],
   "source": [
    "import folium\n",
    "\n",
    "def create_popup_content(row):\n",
    "    popup_content = \"\"\n",
    "    if not pd.isna(row['name_city']):\n",
    "        popup_content += f\"{row['name_city']}, \"\n",
    "    # if not pd.isna(row['alternate_name_admin1']):\n",
    "    #     popup_content += f\"{row['alternate_name_admin1']}, \"\n",
    "    # if not pd.isna(row['alternate_name_country']):\n",
    "    #     popup_content += f\"{row['alternate_name_country']}, \"\n",
    "    # if not pd.isna(row['feature_code']):\n",
    "    #     popup_content += f\"{row['feature_code']}, \" \n",
    "    \n",
    "    # Format population with commas\n",
    "    popup_content += f\"Population: {int(row['population']):,}, \"  \n",
    "\n",
    "    # Add radius\n",
    "    radius = calculate_radius(row['population'])\n",
    "    # Format radius with commas\n",
    "    popup_content += f\"Radius: {radius:,} meters\"  \n",
    "\n",
    "    return popup_content.rstrip(\", \")  # Remove trailing comma and space\n",
    "\n",
    "# Create a map centered on a specific location\n",
    "m = folium.Map(location=[47.4979, 19.0402], zoom_start=10)  # Centered on Budapest\n",
    "\n",
    "# Add markers with circles for each city\n",
    "for index, row in cities_with_country_admin1_geocodes.iterrows():\n",
    "    folium.Circle(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        radius=row['estimated_radius'],  # Example radius in meters\n",
    "        popup=create_popup_content(row),\n",
    "        color=\"blue\",\n",
    "        fill=True,\n",
    "        fill_color=\"blue\"\n",
    "    ).add_to(m)\n",
    "\n",
    "# Save the map as an HTML file\n",
    "m.save(\"cities_map.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
